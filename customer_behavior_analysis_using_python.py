# -*- coding: utf-8 -*-
"""Customer Behavior Analysis using Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zx0P4t9fbfrOAuvyJeagTq_nM26kbFVW

Analysis of customer behavior is essential for any company seeking to understand its ideal customer base. This process not only enhances a business's comprehension of its clientele but also streamlines the customization of products to align with the distinct needs, behaviors, and concerns of various customer segments.

Within our current dataset comprising information from 2240 customers, we've collected data on crucial aspects such as educational level, marital status, year of birth, expenditure on our products, and purchase modes over the past two years. The focus of our investigation lies in unraveling the factors influencing customer spending patterns. This exploration involves addressing pivotal questions, including identifying the most favored purchasing modes among customers and delving into the impact of educational level, age range, and marital status on expenditures. Furthermore, our analysis extends to understanding the types of products that customers are most inclined to purchase.
"""

# Import libraries
 import pandas as pd
 import numpy as np
 import seaborn as sea
 import scipy.stats as stats
 import matplotlib
 import matplotlib.pyplot as plt
 from matplotlib import patches
 !pip install skimpy
 !pip install pingouin
 import pingouin as pg

 from datetime import date
 plt.figure(figsize=(7, 5))

# Upload dataset
from google.colab import files
uploaded = files.upload()

# Loading dataset
import io
df = pd.read_csv(io.BytesIO(uploaded['marketing_campaign.csv']),sep="\t")

df.head()

df.shape

"""# **Data Cleaning and Transformation**
In this phase, we address missing values in the data and perform data transformation to ensure it is in the appropriate format.

To begin, we categorize values in the 'Education' and 'Marital_Status' columns. Additionally, we calculate and categorize ages based on the year of birth.
"""

# Remove missing value (null)
df.dropna(inplace=True)

print(df['Education'].unique())
print(df['Marital_Status'].unique())

customer_data = df.copy()

# Replacing 2nd Cycle with Master degree in Education column
replace_vals = '2n Cycle'
customer_data['Education'] = customer_data['Education'].apply(lambda val:'Master' if val in replace_vals else val )

# Grouping the categories in Marital_Status column into 2 categories such as 'single' and 'in relationship'
replace_vals = ['Together', 'Married']
customer_data['Marital_Status'] = customer_data['Marital_Status'].apply(lambda val: 'In relationship' if val in replace_vals else 'Single')

print(customer_data['Education'].unique())
print(customer_data['Marital_Status'].unique())

# Calculating current year
current_year = date.today().year

# Adding column "Age"
customer_data['Age'] = current_year - customer_data['Year_Birth']

# Grouping Age
customer_data['Age_Group'] = customer_data['Age'].apply(lambda x: 'Children' if x <= 16 else (
    'Youth' if 17 <= x <= 30 else (
    'Middle Aged' if 30 < x <=  45 else 'Old'
    )
))

# Drop some tables
customer_data = customer_data.drop(['ID', 'Year_Birth', 'Z_CostContact', 'Z_Revenue', 'Complain'], axis=1)

# Calculating Total Spent by customers
customer_data['Total_Spent'] = customer_data['MntWines'] + customer_data['MntFruits'] + customer_data['MntMeatProducts'] + customer_data['MntFishProducts'] + customer_data['MntSweetProducts'] +customer_data['MntGoldProds']

# Calculating total number of purchase made by customers
customer_data['Total_Purchases'] = customer_data['NumDealsPurchases' ] + customer_data['NumWebPurchases'] + customer_data['NumCatalogPurchases'] + customer_data['NumStorePurchases']

# Calculating the enrollment year of customer
customer_data['Dt_Customer'] = pd.to_datetime(customer_data['Dt_Customer'], errors='coerce')
customer_data['Enrollment_Year'] = customer_data['Dt_Customer'].dt.year
customer_data['Seniority'] = current_year - customer_data['Enrollment_Year']

# Calculating total number of accepted offers for each customer
customer_data['Total_Offers'] = customer_data['AcceptedCmp1'] + customer_data['AcceptedCmp2'] + customer_data['AcceptedCmp3'] + customer_data['AcceptedCmp4'] + customer_data['AcceptedCmp5']

customer_data.head()

"""# **Exploratory Data Analysis**

**Age Range of Customer**
"""

# Calculating percentage of age range
age_range = customer_data.groupby('Age_Group').size().reset_index(name='num')
age_range['percentage'] = (age_range['num'] * 100 / age_range['num'].sum()).round(2)

print(age_range)

#Plotting age range distribution
labels = age_range['Age_Group']
sizes = age_range['num']
colors = ["#41B7C4", "#CCEDB1", "#F5CA63"]

# Create a pie chart
plt.figure(figsize=(7, 5))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.2f%%', startangle=140)
plt.title('Age Range Distribution')
plt.legend(bbox_to_anchor=(1, 1), loc='upper left', borderaxespad=0)
plt.show()

"""*Observation:*

*   75.27% of customers are aged above 45
*   24.05% of them are Middle Aged (between 30 and 45)
*   There is no significant amount of Children and Youth customer

**Education Level of Customer**
"""

# Calculating percentage of education level
education_range = customer_data.groupby('Education').size().reset_index(name='num')
education_range['percentage'] = (education_range['num'] * 100 / age_range['num'].sum()).round(2)

print(education_range)

# Plotting Education Level Distribution
labels = education_range['Education']
sizes = education_range['num']
colors = ["#41B7C4", "#CCEDB1", "#F5CA63", "#808A87"]

# Create a pie chart
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.2f%%', startangle=140)
plt.title('Education Level Distribution')
plt.legend(bbox_to_anchor=(1, 1), loc='upper left', borderaxespad=0)
plt.show()

"""*Observation:*

*  50.36% of customers are Graduation
*  25.50% of customers are Master
*  21.71% of customers are PhD
*  Slightly amount of customers are Basic

**Marital Level of Customer**
"""

# Calculating percentage of marital status
marital_range = customer_data.groupby('Marital_Status').size().reset_index(name='num')
marital_range['percentage'] = (education_range['num'] * 100 / age_range['num'].sum()).round(2)

print(marital_range)

# Plotting Marital Status Distribution
labels = marital_range['Marital_Status']
sizes = marital_range['num']
colors = ["#41B7C4", "#CCEDB1"]

# Create a pie chart
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.2f%%', startangle=140)
plt.title('Marital Status Distribution')
plt.legend(bbox_to_anchor=(1, 1), loc='upper left', borderaxespad=0)
plt.show()

"""*Observation:*

*   64.53% of customers are in relationship
*   35.47% of them are single

**Total Spent Variable**
"""

from matplotlib.ticker import PercentFormatter

plt.hist(customer_data['Total_Spent'], weights=np.ones(len(customer_data['Total_Spent'])) / len(customer_data['Total_Spent'])*100, bins=5, alpha=0.5, color='lightblue', edgecolor='darkblue')
# Customize the x-axis ticks
plt.xticks(np.arange(0, 2501, 500))

# Customize the y-axis
plt.gca().yaxis.set_major_formatter(PercentFormatter())
# Labeling
plt.xlabel('Total Spent')
plt.ylabel('Probability')

plt.show()

"""*Observation :*

*   58% of customers have spending below 500 dollars
*   18% of customer have spending between 500 dollars and 1000 dollars
*   16% of customer have spending between 1000 dollars and 1500 dollars
*   Spending more than 2000 dollars seems to be rare
"""

# Sort the data for ECDF plotting
sorted_data = np.sort(customer_data['Total_Spent'])

# Calculate the ECDF
y = np.arange(1, len(sorted_data) + 1) / len(sorted_data)

# Create the ECDF plot
plt.figure(figsize=(8, 6))
plt.step(sorted_data, y, color="#00AFBB", linestyle='--', linewidth=1)

# Customize the y-axis ticks
plt.yticks(np.arange(0, 1.1, 0.1))

# Labeling
plt.xlabel('Total Spent')
plt.ylabel('ECDF')

# Apply a white background theme
plt.style.use('ggplot')

# Show the plot
plt.show()

"""*Observation :*

*   To support the previous graph, more than 95% of customer spent less than 2000 dollars
*   55% of customer spending less than 500 dollars

**TOTAL INCOME VARIABLE**
"""

plt.hist(customer_data['Income'], weights=np.ones(len(customer_data['Income'])) / len(customer_data['Income'])*100,  range=(0,200000), bins=5, alpha=0.5, color='lightblue', edgecolor='darkblue')

# Customize the x-axis limits and ticks
plt.xlim(0, 200000)
plt.xticks(np.arange(0, 200001, 20000))

# Customize the y-axis
plt.gca().yaxis.set_major_formatter(PercentFormatter())

# Labeling
plt.xlabel('Income')
plt.ylabel('Probability')

plt.show()

"""*Observation :*

*   58% of customer earn between 40k and 80k dollars annually
*   32% of customer spending less than 500 dollars
*   32% of customer spending less than 500 dollars

"""

# Sort the data for CDF plotting
sorted_data = np.sort(customer_data['Income'])

# Calculate the CDF
y = np.arange(1, len(sorted_data) + 1) / len(sorted_data)

# Create the CDF plot
plt.step(sorted_data, y, color="#00AFBB", linestyle='--', linewidth=1)

# Customize the x-axis and y-axis ticks
plt.xticks(np.arange(0,200000, step=20000))
plt.yticks(np.arange(0, 1.1, 0.1))
plt.xlim(0, 200000)

# Labeling
plt.xlabel('Income')
plt.ylabel('CDF')
plt.title('Cumulative Distribution Function (CDF) Plot of Income')

# Apply a white background theme
plt.style.use('ggplot')

# Show the plot
plt.show()

"""*Observation:*

More than 95% customer earn less the 100k dollars.

**Correlation between Income and Expense Varibale (Total Spent)**
"""

import scipy.stats as stats

# Perform the correlation test
correlation_coefficient, p_value = stats.pearsonr(customer_data['Income'], customer_data['Total_Spent'])

# Print the results
print("Correlation Coefficient:", correlation_coefficient)
print("P-value:", p_value)

sea.set(style="whitegrid")

# Create the scatterplot
sea.scatterplot(data=customer_data, x="Income", y="Total_Spent", color="green", edgecolor="black", linewidth=0.5, s=40)

# Set x-axis and y-axis limits and ticks
plt.xlim(0, 200000)
plt.xticks(range(0, 200001, 20000))
plt.ylim(0, 4000)
plt.yticks(range(0, 4001, 1000))

# Add a linear regression line
sea.regplot(data=customer_data, x="Income", y="Total_Spent", scatter=False, color="darkred", line_kws={"linestyle": "--"})

# Labeling
plt.xlabel('Income')
plt.ylabel('Total Spent')
plt.title('Correlation between Income and Total Spent')

# Display the plot
plt.show()

"""*Observation :*

The correlation coefficient between income and total spent variable is 0.66. This shows that, income and the expense variable have a moderate correlation postively.

**Average amount spend by customers with respect to Education**
"""

sea.set(style="whitegrid")

# Create the box plot
sea.boxplot(data=customer_data, x="Education", y="Total_Spent", palette="Set2")

# Labeling
plt.xlabel('Education')
plt.ylabel('Total Spent')

# Display the plot
plt.show()

"""*Observation :*

The average amount spend  by the customer with basic education customer is lower than other. Customer with PhD customer is the highest average amount spend.

**Average amount spend by customers with respect to Marital Status**
"""

sea.set(style="whitegrid")

# Create the box plot
sea.boxplot(data=customer_data, x="Marital_Status", y="Total_Spent", palette="Set2")

# Labeling
plt.xlabel('Marital Status')
plt.ylabel('Total Spent')

# Display the plot
plt.show()

"""*Observation :*

The average amount spend  by the customer who are single is slighly bigger than customer who in relationship.

**Average amount spend by customers with respect to Age Range**
"""

sea.set(style="whitegrid")

# Create the box plot
sea.boxplot(data=customer_data, x="Age_Group", y="Total_Spent", palette="Set2")

# Labeling
plt.xlabel('Age Group')
plt.ylabel('Total Spent')

# Display the plot
plt.show()

"""*Observation :*

Customer who aged between 17 and 30 spend more than the other age range.

**Average amount spend by customers with respect to Products**
"""

# Select columns that start with "Mnt"
products_df = customer_data.filter(like="Mnt")

# Create a DataFrame with the Product_Name column
Product_Name = pd.DataFrame({'Product_Name': ['Wine', 'Fruit', 'Meat', 'Fish', 'Sweet', 'Gold'] * 2240})

# Create a DataFrame with Total_Spent
Total_Spent = pd.DataFrame({'Total_Spent': products_df.values.ravel()})

# Combine Product_Name and Total_Spent
products_df = pd.concat([Product_Name, Total_Spent], axis=1)

# Display the first 3 rows
print(products_df.head(3))

sea.set(style="whitegrid")

# Create the box plot
sea.boxplot(data=products_df, x="Product_Name", y="Total_Spent", palette="Set2", fliersize=5, linewidth=1)

# Rotate x-axis labels for better visibility
#plt.xticks(rotation=45)

# Labeling
plt.xlabel('Product Name')
plt.ylabel('Total Spent')

# Display the plot
plt.show()

"""**Observations:**

Amount spend on prducts such as Wine & Meat seem to be high than the other products.

**CDF & PDF of Customer Purchases in last 2 years**
"""

# Plotting histogram
plt.hist(customer_data['Total_Purchases'], weights=np.ones(len(customer_data['Total_Purchases'])) / len(customer_data['Total_Purchases'])*100, bins=6, alpha=0.5, color='lightblue', edgecolor='darkblue')

# Customize the y-axis
plt.gca().yaxis.set_major_formatter(PercentFormatter())

# Determine the lower value of Total_Purchases
lower_limit = customer_data['Total_Purchases'].min()

# Set x-axis limits and ticks
plt.xlim(lower_limit, 50)
plt.xticks(range(lower_limit, 51, 5))

# Labeling
plt.xlabel('Total Purhcases')
plt.ylabel('Probability')

plt.show()

"""Observation:


*   29% of Customer purchase between 15 to 22 times
*   25% of Customer purchase below 7 times
*   22.5% of Customer purchase between 7 to 15 times
*   21% of Customer purchase between 22 to 29 times
"""

# Sort the data for CDF plotting
sorted_data = np.sort(customer_data['Total_Purchases'])

# Calculate the CDF
y = np.arange(1, len(sorted_data) + 1) / len(sorted_data)

# Create the CDF plot
plt.figure(figsize=(8, 6))
plt.step(sorted_data, y, color="#00AFBB", linestyle='--', linewidth=1)

# Customize the x-axis and y-axis ticks
plt.xticks(np.arange(0,50, step=5))
plt.yticks(np.arange(0, 1.1, 0.1))

# Labeling
plt.xlabel('Total Purchases')
plt.ylabel('CDF')

# Apply a white background theme
plt.style.use('ggplot')

# Show the plot
plt.show()

"""Observation:

*   88% of Customers purchase less than 25 times
*   48% of Customers purchase less than 15 times

**Preferred Purchase Mode by Customers**
"""

# Assuming you have a DataFrame named main_df
# Select columns that start with "Num" and end with "Purchases"
purchase_df = customer_data.filter(like='Num').filter(like='Purchases')

# Create a DataFrame for Purchase_Name
Purchase_Name = pd.DataFrame({'Purchase_Name': ['Deal', 'Web', 'Catalog', 'Store'] * 2240})

# Create a DataFrame for Total_Purchases by unlisting purchase_df
Total_Purchases = pd.DataFrame({'Total_Purchases': purchase_df.values.flatten()}, index=None)

# Combine Purchase_Name and Total_Purchases into a single DataFrame
purchase_df = pd.concat([Purchase_Name, Total_Purchases], axis=1)

# Display the first 3 rows of the resulting DataFrame
print(purchase_df.head(3))

sea.set(style="whitegrid")

# Create the box plot
sea.boxplot(data=purchase_df, x="Purchase_Name", y="Total_Purchases", palette="Set2", fliersize=5, linewidth=1)

# Rotate x-axis labels for better visibility
#plt.xticks(rotation=45)

# Display the plot
plt.xlabel("Purchase Name")
plt.ylabel("Total Purchases")
plt.title("Boxplot of Total Purchases by Purchase Name")
plt.show()

# Assuming you have a DataFrame named purchase_df
# Group by 'Purchase_Name' and calculate the sum of 'Total_Purchases' for each group
grouped_df = purchase_df.groupby('Purchase_Name')['Total_Purchases'].sum().reset_index()

# Calculate the percentage and round it to 2 decimal places
grouped_df['percentage'] = (grouped_df['Total_Purchases'] * 100 / grouped_df['Total_Purchases'].sum()).round(2)

# Display the resulting DataFrame
print(grouped_df)

"""Observation:
*   Purchase made in Stores is the most preferred purchase method for customers with percentage in 38.98%.
*   Another preferred purchase method is made in Web with percentage 27.45%

**Average Web Visit for last month**
"""

# Assuming you have a DataFrame named main_df
# Calculate the average of the 'NumWebVisitsMonth' column
average_web_visit = customer_data['NumWebVisitsMonth'].mean()

# Display the result
print("Average_Web_Visit:", average_web_visit)

# Plotting histogram
plt.hist(customer_data['NumWebVisitsMonth'], weights=np.ones(len(customer_data['Total_Purchases'])) / len(customer_data['Total_Purchases'])*100, bins=5, alpha=0.5, color='lightblue', edgecolor='darkblue')

# Customize the y-axis
plt.gca().yaxis.set_major_formatter(PercentFormatter())

# Determine the lower value of Total_Purchases
lower_limit = customer_data['NumWebVisitsMonth'].min()

# Set x-axis limits and ticks
#plt.xlim(lower_limit, 20)
plt.xticks(range(lower_limit, 21, 1))


# Labeling
plt.xlabel('No of Web Visits')
plt.ylabel('Probability')

plt.show()

"""*Observations:*

*   58% of customers visit the company's website 4 to 8 times
*   25% of customers visit the company's website below 4 times
*   Around 20% of customers visit the company's website 8 to 12 times
*   The chances of visiting the website more than 12 times seem to be rare
"""

# Sort the data for CDF plotting
sorted_purchases = np.sort(customer_data['NumWebVisitsMonth'])

# Calculate the CDF
y = np.arange(1, len(sorted_purchases) + 1) / len(sorted_purchases)

# Create the CDF plot
plt.figure(figsize=(8, 6))
plt.step(sorted_purchases, y, color="#00AFBB", linestyle='--', linewidth=1)

# Customize the x-axis and y-axis ticks
plt.xticks(np.arange(0,20, step=1))
plt.yticks(np.arange(0, 1.1, 0.1))

# Labeling
plt.xlabel('Num Web Visits Month')
plt.ylabel('CDF')

# Apply a white background theme
plt.style.use('ggplot')

# Show the plot
plt.show()

"""Observations:

95% of customers visits the website less than 9 times.

**Percentage of Customer's Enrollment in Years**
"""

seniority = customer_data[['Seniority']].rename(columns={'Seniority': 'Total_Years'})

# Group by 'Total_Years' and calculate the count of each group
seniority = seniority.groupby('Total_Years').size().reset_index(name='num')

# Calculate the percentage and round it to 2 decimal places
seniority['percentage'] = (seniority['num'] * 100 / seniority['num'].sum()).round(2)

# Convert 'Total_Years' to a categorical variable (factor)
seniority['Total_Years'] = seniority['Total_Years'].astype('category')

# Display the resulting DataFrame
print(seniority)

sea.set(style="whitegrid")  # Set the plot style

# Create a bar plot using seaborn
sea.barplot(x='Total_Years', y='num', data=seniority, palette=["#41B7C4", "#CCEDB1", "#F5CA63"])

# Add text labels with percentages
for index, row in seniority.iterrows():
    plt.text(index, row['num'], f"{row['num']} ({row['percentage']}%)", va='bottom', ha='center')

# Customize labels and scales
plt.xlabel('No of Years')
plt.ylabel('Count')

# Show the plot
plt.show()

# Plotting Education Level Distribution
# Data
labels = seniority['Total_Years']
sizes = seniority['num']
colors = ["#41B7C4", "#CCEDB1", "#F5CA63", "#808A87"]

# Create a pie chart
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.2f%%', startangle=140)
#plt.title('Education Level Distribution')
plt.legend(bbox_to_anchor=(1, 1), loc='upper left', borderaxespad=0)
plt.show()

# Group by 'Enrollment_Year' and calculate the sum of 'Total_Spent' for each group
grouped_df = customer_data.groupby('Enrollment_Year')['Total_Spent'].sum().reset_index()

# Calculate the percentage and round it to 2 decimal places
grouped_df['percentage'] = (grouped_df['Total_Spent'] * 100 / grouped_df['Total_Spent'].sum()).round(2)

# Display the resulting DataFrame
print(grouped_df)

"""*Observations:*

*   52.9% of Customers have been with the company for 10 years
*   52% of total amount spend by customers who have enrolled in 2013

**Correlation between Income and Number of Purchases**
"""

# Perform the correlation test
correlation_coefficient, p_value = stats.pearsonr(customer_data['Income'], customer_data['Total_Purchases'])

# Print the results
print("Correlation Coefficient:", correlation_coefficient)
print("P-value:", p_value)

sea.set(style="whitegrid")

# Create the scatterplot
sea.scatterplot(data=customer_data, x="Income", y="Total_Purchases", color="green", edgecolor="black", linewidth=0.5, s=40)

# Set x-axis limits and ticks
plt.xlim(0, 200000)
plt.xticks(range(0, 200001, 20000))

# Set y-axis limits and ticks
#plt.ylim(0, 4000)
#plt.yticks(range(0, 4001, 1000))

# Add a linear regression line
sea.regplot(data=customer_data, x="Income", y="Total_Purchases", scatter=False, color="darkred", line_kws={"linestyle": "--"})

# Display the plot
plt.show()

"""*Observations:*

The correlation coeffieienct between income and total spent variable is 0.56. This shows that, income and total purchases have a moderate correlation postively.

**Performance of Campaigns**
"""

offers = customer_data[['Total_Offers']].rename(columns={'Total_Offers': 'Offers_Total'})

# Group by 'Total_Years' and calculate the count of each group
offers = offers.groupby('Offers_Total').size().reset_index(name='num')

# Calculate the percentage and round it to 2 decimal places
offers['percentage'] = (offers['num'] * 100 / seniority['num'].sum()).round(2)

# Convert 'Total_Years' to a categorical variable (factor)
offers['Offers_Total'] = offers['Offers_Total'].astype('category')

# Display the resulting DataFrame
print(offers)

sea.set(style="whitegrid")  # Set the plot style

# Create a bar plot using seaborn
sea.barplot(x='Offers_Total', y='num', data=offers)

# Add text labels with percentages
for index, row in offers.iterrows():
    plt.text(index, row['num'], f"{row['num']} ({row['percentage']}%)", va='bottom', ha='center')

# Customize labels and scales
plt.xlabel('Accepted No of Years')
plt.ylabel('Count')

# Show the plot
plt.show()

cmp_df = customer_data[['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']]

# Creating the Campaign column
Campaign = pd.DataFrame({'Campaign': ['Campaign 1', 'Campaign 2', 'Campaign 3', 'Campaign 4', 'Campaign 5'] * 2240})

# Creating the No_Of_Offers column
No_Of_Offers = pd.DataFrame({'No_Of_Offers': cmp_df.values.flatten()})

# Combining Campaign and No_Of_Offers into cmp_df
cmp_df = pd.concat([Campaign, No_Of_Offers], axis=1)

# Display the first 3 rows
print(cmp_df.head(3))

# Filter rows where No_Of_Offers is equal to 1
filtered_df = cmp_df[cmp_df['No_Of_Offers'] == 1]

# Group by Campaign and calculate the count of rows in each group
grouped_df = filtered_df.groupby('Campaign').size().reset_index(name='num')

# Calculate the percentage
grouped_df['percentage'] = (grouped_df['num'] * 100 / grouped_df['num'].sum()).round(2)

# Display the result
print(grouped_df)

sea.set(style="whitegrid")  # Set the plot style

# Create a bar plot using seaborn
sea.barplot(x='Campaign', y='num', data=grouped_df)

# Add text labels with percentages
for index, row in grouped_df.iterrows():
    plt.text(index, row['num'], f"{row['num']} ({row['percentage']}%)", va='bottom', ha='center')

# Customize labels and scales
plt.xlabel('Campaign')
plt.ylabel('Count')

# Show the plot
plt.show()

result_df = customer_data.groupby('Total_Offers').agg(avg_spend=('Income', 'median')).reset_index().round(0)

print(result_df)

"""*Observations:*

*   79.29% of Customers accepted no offers in the campaigns
*   14.58% of Customers accepted only one offer in the campaigns
*   3.66% of Customers accepted only two offer in the campaigns
*   1.99% of Customers accepted only three offer in the campaigns
*   0.50% of Customers accepted only four offer in the campaigns
*   There is no customers who accepted all the five offers in the campaigns conducted

Additionally, while a significant number of customers embraced the offers in the fourth campaign, a comparably small group accepted the offers during the second campaign. This suggests a positive correlation between the number of accepted offers and the income level of our customers.

# **Statistical Testing**

**Chi-Square Test of Independence**

A Chi-Square Test of Independence is used to determine whether or not there is a significant association between two categorical variables.In our statistical analysis, we will employ the Chi-Square Test of Independence with a significance level set at alpha = 0.05. We will calculate the p-value, and our decision rule will be as follows: If the p-value is less than 0.05, we will reject the null hypothesis (H0), indicating the presence of a statistically significant relationship between variables. Conversely, if the p-value is greater than or equal to 0.05, we will fail to reject the null hypothesis, suggesting no statistically significant relationship between the variables.
"""

single = customer_data[customer_data['Marital_Status'] == 'Single'].groupby('Education')['Total_Spent'].mean().round(2).reset_index()
single.rename(columns={'Total_Spent': 'single'}, inplace=True)

in_relationship = customer_data[customer_data['Marital_Status'] == 'In relationship'].groupby('Education')['Total_Spent'].mean().round(2).reset_index()
in_relationship.rename(columns={'Total_Spent': 'in_relationship'}, inplace=True)

joined_df = pd.merge(single, in_relationship, on='Education')
joined_df.index = ['Basic', 'Graduation', 'Master', 'PhD']
joined_df = joined_df[['single', 'in_relationship']]
print(joined_df)

# Chi-Square test of Independence
stats.chi2_contingency(joined_df)

"""The p-value of 6.496225989239669e-06 is significantly less than the alpha value of 0.05, providing strong evidence to reject the null hypothesis. This suggests that there is a statistically significant relationship or effect in our analysis."""

minDim = min(customer_data.shape)-1
#calculate Cramer's V
V = np.sqrt((26.796099688305084/2240) / minDim)
print(V)

"""Cramer’s V is a measure of the strength of association between two nominal variables.
It ranges from 0 to 1 where:
0 indicates no association between the two variables.
1 indicates a strong association between the two variables.

The result we found indicates weak association between the education and marital status of customers

"""

single = customer_data[customer_data['Marital_Status'] == 'Single'].groupby('Age_Group')['Total_Spent'].mean().round(2).reset_index()
single.rename(columns={'Total_Spent': 'single'}, inplace=True)

in_relationship = customer_data[customer_data['Marital_Status'] == 'In relationship'].groupby('Age_Group')['Total_Spent'].mean().round(2).reset_index()
in_relationship.rename(columns={'Total_Spent': 'in_relationship'}, inplace=True)

joined_df1 = pd.merge(single, in_relationship, on='Age_Group')
joined_df1.index = ['Middle Aged', 'Old', 'Youth']
joined_df1 = joined_df1[['single', 'in_relationship']]
print(joined_df1)

# Chi-Square test of Independence
stats.chi2_contingency(joined_df1)

"""The p-value of 2.474395067603421e-13 is significantly less than the alpha value of 0.05, providing strong evidence to reject the null hypothesis. This suggests that there is a statistically significant relationship or effect in our analysis."""

minDim_1 = min(customer_data.shape)-1
#calculate Cramer's V
V1 = np.sqrt((58.05522051981321/2240) / minDim_1)
print(V1)

"""This indicates weak relationship between marital status and age range of customers.

**One-Way ANOVA ("analysis of variance")**

A one-way ANOVA is a statistical method used to compare the means of three or more groups to determine if there are any statistically significant differences among them. To address this, the Tukey Test is employed after ANOVA, offering insights into specific group differences. However, ANOVA results can be compromised if group variances are unequal.

In such cases, Welch's ANOVA is a robust alternative, allowing the use of the Games-Howell multiple comparisons method. Key assumptions for one-way ANOVA include normality (often valid for large samples), independence of groups, and equality of variances, which can be assessed with Levene's Test.

The analysis aims to determine if educational levels influence customer spending, where the null hypothesis suggests equal spending, and the alternative hypothesis posits differing spending patterns among customers.
"""

from scipy.stats import f_oneway

# Perform Levene's test for homogeneity of variances
stats.levene(*[customer_data[customer_data['Education'] == category]['Total_Spent'] for category in customer_data['Education'].unique()]
)

"""Since that the p-value 3.61478744078149e-15 is below the 5% significance level (α = .05), we reject the null hypothesis, indicating unequal variances among educational levels. This deviation from the assumption necessary for one-way ANOVA necessitates the application of Welch's ANOVA, a robust alternative that does not hinge on the equality of variances."""

# Perform Welch's ANOVA test
stats.f_oneway(*[customer_data[customer_data['Education'] == category]['Total_Spent'] for category in customer_data['Education'].unique()]
)

"""Since the p-value (which is 5.3198013144921505e-11) is less than the significance level(α = .05), we reject the null
hypothesis at the 5% significance level.
"""

# Perform Games-Howell post hoc test
pg.pairwise_gameshowell(data=customer_data, dv='Total_Spent', between='Education')

"""It is evident that there is a statistically significant diffrence between the amount spend
by the customers with primary and university education.

*Are there any differences between the amount spent when
considering marital status of customers?*

**Overview of Two Sample t test**
"""

# Perform Levene's test for homogeneity of variances
stats.levene(*[customer_data[customer_data['Marital_Status'] == category]['Total_Spent'] for category in customer_data['Marital_Status'].unique()]
)

"""Since the p-value = 0.2655 is greater than the significance level(α = .05), we fail to reject the null hypothesis at
the 5% significance level. That is, variances are equal across the marital status of customers. Thus, by
satisfying all the conditions, we can use 2 sample t test.
"""

group1 = customer_data.query('Marital_Status == "Single"')['Total_Spent']
group2 = customer_data.query('Marital_Status == "In relationship"')['Total_Spent']

stats.ttest_ind(group1, group2, equal_var=True)

"""Since the p-value = 0.3626 is greater than the significance level(α = .05), we fail to reject the null hypothesis at
the 5% significance level.

There is not sufficient evidence to support the claim that amount spend by customers
varies with respect to their marital status.
Thus, Marital Status, on its own, does not consititute a diffrence in the amount spend by customers.

*Are there any differences between the amount spent when
considering age group of customers?*
"""

# Perform Levene's test for homogeneity of variances
stats.levene(*[customer_data[customer_data['Age_Group'] == category]['Total_Spent'] for category in customer_data['Age_Group'].unique()]
)

"""Since the p-value = 0.7469 is greater than the significance level(α = .05), we fail to reject the null hypothesis at
the 5% significance level. That is, variances are equal across the age group of customers. Thus, by
satisfying all the conditions, we can use one way ANOVA.
"""

# Perform a one-way ANOVA
model = stats.f_oneway(*[group['Total_Spent'] for name, group in customer_data.groupby('Age_Group')])
print(model)

"""Since the p-value = 0.000735 is less than the significance level(α = .05), we reject the null hypothesis at the
5% significance level.

There is sufficient evidence to support the claim that amount spend by customers
varies with respect to their age group.
Thus, Age, on its own, contitute a diffrence in the amount spend by customers
"""

pg.pairwise_tukey(data=customer_data,dv='Total_Spent', between='Age_Group')

"""It is evident that there is a statistically significant diffrence between the amount spend
by the middle aged (between 30 and 45) customers than the other age group of
customers

*Are there any differences between the amount spent when
considering the products we sell?*
"""

products_df['Customer_Id'] = list(range(1, 2240 + 1)) * 6

products_df['Product_Name'] = pd.Categorical(products_df['Product_Name'])

# Print the first few rows of the DataFrame to check the changes
print(products_df.head())

"""**Overview of Friedman Test**

The Friedman Test serves as a non-parametric alternative to Repeated Measures ANOVA, assessing if there's a significant difference among means in three or more groups with the same subjects. Post-hoc testing involves pairwise Wilcoxon rank sum tests with a Bonferroni correction to address multiple comparisons. Assumptions include the continuity of the variable, random sampling, and sufficient data, emphasizing the need for a continuous variable, simple random samples, and an adequate sample size relative to expected differences across groups. The Bonferroni correction is employed to mitigate the increased risk of Type I errors associated with multiple statistical tests.
"""

from scipy.stats import friedmanchisquare

result = friedmanchisquare(*[group['Total_Spent'].values for name, group in products_df.groupby('Product_Name')])
result

"""Since the p-value is less than the significance level(α = .05), we reject the null hypothesis at the 5%
significance level. we can reject the null hypothesis that the average amount spend by customers is the same
for all products.
"""

# Perform pairwise Wilcoxon test with Bonferroni correction
result = pg.pairwise_ttests(dv='Total_Spent', between='Product_Name', padjust='bonferroni', data=products_df)

# Display the result
print(result)

"""*   The comparison between 'Wine' and 'Fruit' as well as 'Fruit' and 'Meat' indicates significant distinctions, supported by very low adjusted p-values.
*   'Fruit' vs. 'Sweet' and 'Fish' vs. 'Gold', show no significant differences after Bonferroni correction, as their adjusted p-values exceed the 0.05 threshold.
*   'Wine' and 'Meat' emerge as top products with statistically significant differences.
*   There is no statistically significant diffrence between the amount spend on the products: Fruits and Sweets.

# **Insights**

*   We must enhance the effectiveness of our campaigns as approximately 79.29% of our customers currently do not accept our offers. Notably, the acceptance of offers demonstrates a positive correlation with customers' income levels. Additionally, 58% of our customer base falls within the annual income range of 40k to 80k dollars. Therefore, optimizing campaigns to feature offers appealing to customers earning below 60k dollars annually is likely to elevate the acceptance rate.
*   Income and the amount spend of customers have a moderate correlation positively.  Similarly, there is a moderate positive correlation between the income level and the total no of purchases made by our customers.
*   Customers with University education spend more than the customers with basic education.
*   Marital Status, on its own, does not constitute any difference in the amount spend by our customers
*   Customer who aged between 17 and 30 spend more than the other age range.
*   The products that most likely customers buy are Wine and Meat. Customers spend equally on
fruits and sweets
*   52% of total amount spend by customers who have enrolled in 2013.
*   38.98% customers prefer to purchase in Stores and 27.45% of customer prefer online purchasing.
*  In last 2 years, 95% of customers visits the website less than 9 times.
"""